from scipy.io import arff
import pandas as pd
from sklearn.model_selection import train_test_split, KFold
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import mean_squared_error, confusion_matrix, accuracy_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
import numpy as np

def load_data(file_path):
    """Carregar e preparar os dados do arquivo .arff."""
    data, _ = arff.loadarff(file_path)
    df = pd.DataFrame(data)
    df['class'] = df['class'].apply(lambda x: x.decode('utf-8'))
    df['class'] = df['class'].map({'tested_positive': 1, 'tested_negative': 0})
    X = df.drop('class', axis=1)
    y = df['class']
    return X, y

def scale_data(X_train, X_test):
    """Normalizar os dados usando StandardScaler."""
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    return X_train_scaled, X_test_scaled

def evaluate_model(X, y, learning_rates, hidden_layer_sizes_list, n_runs=30):
    """Executar e avaliar o modelo com validação cruzada."""
    results = []
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    
    for run in range(n_runs):
        print(f"Execução {run + 1}/{n_runs}")
        
        for lr in learning_rates:
            for hidden_layer_sizes in hidden_layer_sizes_list:
                mse_list = []
                conf_matrices = []

                for train_index, test_index in kf.split(X):
                    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
                    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
                    
                    X_train_scaled, X_test_scaled = scale_data(X_train, X_test)

                    mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, learning_rate_init=lr, max_iter=1000, random_state=run)
                    mlp.fit(X_train_scaled, y_train)
                    
                    y_pred = mlp.predict(X_test_scaled)

                    mse = mean_squared_error(y_test, y_pred)
                    conf_matrix = confusion_matrix(y_test, y_pred)
                    
                    mse_list.append(mse)
                    conf_matrices.append(conf_matrix)

                # Média dos MSEs e matrizes de confusão
                mse_mean = np.mean(mse_list)
                conf_matrix_mean = np.mean(np.array(conf_matrices), axis=0)

                results.append({
                    'iteration': run + 1,
                    'learning_rate': lr,
                    'hidden_layer_sizes': hidden_layer_sizes,
                    'mse_mean': mse_mean,
                    'conf_matrix_mean': conf_matrix_mean.tolist()  # Converter para lista para salvar no CSV
                })

    return results

def summarize_results(results):
    """Converter os resultados para DataFrame e imprimir o resumo estatístico."""
    results_df = pd.DataFrame(results)
    
    summary_stats = results_df.groupby(['learning_rate', 'hidden_layer_sizes']).agg(
        mse_mean=('mse_mean', 'mean'),
        mse_std=('mse_mean', 'std'),
        conf_matrix_mean=('conf_matrix_mean', lambda x: np.mean(np.array(x.tolist()), axis=0))
    ).reset_index()
    
    return summary_stats

def print_summary(summary_stats):
    """Imprimir o resumo detalhado dos resultados."""
    print("\nResumo Estatístico Detalhado:")
    for _, row in summary_stats.iterrows():
        lr = row['learning_rate']
        hls = row['hidden_layer_sizes']
        mse_mean = row['mse_mean']
        mse_std = row['mse_std']
        conf_matrix_mean = row['conf_matrix_mean']
        
        print(f"\nTaxa de Aprendizado: {lr}")
        print(f"Tamanho da Camada Escondida: {hls}")
        print(f"Erro Médio Quadrático (MSE) - Média: {mse_mean:.4f}, Desvio Padrão: {mse_std:.4f}")
        print("Matriz de Confusão Média:")
        print(f"[[{int(conf_matrix_mean[0, 0])}, {int(conf_matrix_mean[0, 1])}]")
        print(f" [ {int(conf_matrix_mean[1, 0])}, {int(conf_matrix_mean[1, 1])}]]")

def main():
    file_path = r"C:\Users\Nayane Jacyara\Documents\Faculdade\sistemaInteligentes\Redes_Neurais\Rede_Neural_Multilayer_Perceptron\diabetes.arff"
    
    X, y = load_data(file_path)
    
    learning_rates = [0.1, 0.01, 0.001]
    hidden_layer_sizes_list = [(3,), (5,), (7,)]
    n_runs = 30
    
    results = evaluate_model(X, y, learning_rates, hidden_layer_sizes_list, n_runs)
    
    summary_stats = summarize_results(results)
    print_summary(summary_stats)
    
    # Salvar resultados em CSV
    results_df = pd.DataFrame(results)
    results_df.to_csv("resultados_experimentos.csv", index=False, quoting=1)

if _name_ == "_main_":
    main()